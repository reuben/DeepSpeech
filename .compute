#!/bin/bash
set -xe

# Add NVIDIA package repositories
curl -LO https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.deb
dpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.deb
curl -L https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub | apt-key add -
apt-get update
curl -LO http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb
dpkg -i nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb
apt-get update

# Install development and runtime libraries (~4GB)
apt-get install -y --no-install-recommends \
    cuda-10-0 \
    libcudnn7=7.6.2.24-1+cuda10.0  \
    libcudnn7-dev=7.6.2.24-1+cuda10.0 \
    libnvinfer5=5.1.5-1+cuda10.0 \
    libnvinfer-dev=5.1.5-1+cuda10.0 \
    python3-venv git-lfs

python3 -m venv /tmp/venv
source /tmp/venv/bin/activate
pip install -U pip setuptools wheel
pip install -r <(grep -v tensorflow requirements.txt)
pip install tensorflow-gpu==1.15.0

apt-get install -y swig

pushd native_client/ctcdecode
    make bindings NUM_PROCESSES=32
    pip install dist/*.whl
    make clean
popd

# set +xe

# curl -L https://iterm2.com/shell_integration/install_shell_integration.sh | bash
# export ITERM_ENABLE_SHELL_INTEGRATION_WITH_TMUX=YES
# source ~/.iterm2_shell_integration.bash

data="${SHARED_DIR}/data"
fis="${data}/LDC/fisher"
swb="${data}/LDC/LDC97S62/swb"
lbs="${data}/OpenSLR/LibriSpeech/librivox"
npr="${data}/NPR/WAMU/sets/v0.3"
cv="${data}/mozilla/CommonVoice/en_1087h_2019-06-12/clips"

python DeepSpeech.py \
  --train_files="${npr}/best-train.csv,${npr}/good-train.csv,${cv}/train.csv,${fis}-train.csv,${swb}-train.csv,${lbs}-train-clean-100.csv,${lbs}-train-clean-360.csv,${lbs}-train-other-500.csv" \
  --train_batch_size=32 \
  --feature_cache=/data/rw/group-ml/ds/quartznet/ds_under_10_seconds_npr=v0.3_win=20ms_step=10ms_filterbanks=80_n_features=64.cache \
  --dev_files="${lbs}-dev-clean.csv" \
  --dev_batch_size=32 \
  --test_files="${lbs}-test-clean.csv" \
  --test_batch_size=32 \
  --checkpoint_dir=../keep \
  --epochs=100 \
  --noearly_stop \
  --learning_rate=0.01 \
  --lr_warm_up=12451 \
  --summary_dir=../keep/summaries \
  --test_output_file=../keep/decodings.json

# python -u DeepSpeech.py \
#   --test_files "${npr}/best-test.csv" \
#   --test_batch_size 32 \
#   --checkpoint_dir "/data/rw/home/quartznet_5407/" \
#   --summary_dir "../keep/summaries" \
#   --test_output_file "../keep/decodings.json" \
#   --cutoff_prob 0.99
